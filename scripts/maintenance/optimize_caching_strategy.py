#!/usr/bin/env python3
"""
CareerCoach.ai Advanced Caching Strategy
Optimize all 3 options for maximum performance and cost savings
"""

import asyncio
import time
import hashlib
from typing import Dict, Any, Optional
from datetime import timedelta
from pathlib import Path
import sys

# Add src directory to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from cache_manager import CacheManager

class OptimizedCacheStrategy:
    """Advanced caching strategy utilizing all 3 options"""
    
    def __init__(self):
        self.cache = CacheManager(
            redis_url="redis://default:EFyvuEe29ZmLwegPCsdl7rUxe9X1WHez@redis-18094.c93.us-east-1-3.ec2.redns.redis-cloud.com:18094",
            langcache_api_key="wy4ECQMI7HUiaZ2sAd_gYbxq8WvRFf7hatMuRN370Hkz7-bFTg-ijsScIJ56AxAz0oIBS9J3twvrzDhmAxJAcNOhaGNsH4CKRHq7q84Un52thaBGNZ8L18NJTXhFo9rp-fkT2rAYBsFc5A9lKkEuKUctr1Mu-oJrb-HfSjbDLnMp_OW9Qmt6FiUDuopjce-FR8q6krpMnOuhj51ZYAKpBR_CJFjevwuQxorSJM1XrQ4r3Cul"
        )
        
    async def cache_job_search_results(self, query: str, location: str, results: Dict) -> str:
        """Cache job search results with intelligent TTL"""
        cache_key = f"jobs:{hashlib.md5(f'{query}:{location}'.encode()).hexdigest()}"
        
        # Cache for 2 hours for fresh job data
        await self.cache.set(cache_key, {
            "query": query,
            "location": location,
            "results": results,
            "cached_at": time.time(),
            "result_count": len(results.get("jobs", []))
        }, expire=timedelta(hours=2))
        
        print(f" Cached {len(results.get('jobs', []))} jobs for '{query}' in {location}")
        return cache_key
    
    async def cache_ai_career_advice(self, user_profile: Dict, question: str, advice: str) -> str:
        """Cache AI career advice with semantic understanding"""
        # Use LangCache for AI responses - longer TTL for advice
        await self.cache.set_ai_response(
            prompt=f"Profile: {user_profile.get('skills', [])} | Question: {question}",
            response=advice,
            model="gpt-4",
            expire=timedelta(days=7)  # Career advice stays relevant longer
        )
        
        print(f" Cached AI advice for user with {len(user_profile.get('skills', []))} skills")
        return "ai_advice_cached"
    
    async def cache_user_session_data(self, user_id: str, session_data: Dict) -> str:
        """Cache user session data with short TTL"""
        cache_key = f"session:{user_id}"
        
        # Short TTL for session data
        await self.cache.set(cache_key, {
            "user_id": user_id,
            "last_search": session_data.get("last_search"),
            "preferences": session_data.get("preferences"),
            "session_start": time.time()
        }, expire=timedelta(hours=1))
        
        print(f"üë§ Cached session data for user {user_id}")
        return cache_key
    
    async def cache_company_insights(self, company_name: str, insights: Dict) -> str:
        """Cache company research with medium TTL"""
        cache_key = f"company:{hashlib.md5(company_name.encode()).hexdigest()}"
        
        # Medium TTL for company data (changes less frequently)
        await self.cache.set(cache_key, {
            "company": company_name,
            "insights": insights,
            "culture_score": insights.get("culture_score"),
            "salary_range": insights.get("salary_range"),
            "cached_at": time.time()
        }, expire=timedelta(hours=24))
        
        print(f"üè¢ Cached insights for {company_name}")
        return cache_key
    
    async def cache_resume_analysis(self, resume_hash: str, analysis: Dict) -> str:
        """Cache resume analysis results"""
        cache_key = f"resume:{resume_hash}"
        
        # Long TTL for resume analysis (unless resume changes)
        await self.cache.set(cache_key, {
            "analysis": analysis,
            "skills_extracted": analysis.get("skills", []),
            "experience_years": analysis.get("experience_years"),
            "industry_match": analysis.get("industry_match"),
            "analyzed_at": time.time()
        }, expire=timedelta(days=30))
        
        print(f"üìÑ Cached resume analysis (hash: {resume_hash[:8]}...)")
        return cache_key

async def demonstrate_optimized_caching():
    """Demonstrate how to use all 3 caching options optimally"""
    print(" CareerCoach.ai Advanced Caching Demonstration")
    print("=" * 60)
    
    strategy = OptimizedCacheStrategy()
    
    # Test job search caching
    print("\n1Ô∏è‚É£ JOB SEARCH CACHING (Redis Cloud)")
    job_results = {
        "jobs": [
            {"title": "Senior Software Engineer", "company": "TechCorp", "salary": "$120k"},
            {"title": "Full Stack Developer", "company": "StartupXYZ", "salary": "$100k"},
            {"title": "DevOps Engineer", "company": "CloudCo", "salary": "$110k"}
        ],
        "total": 3
    }
    await strategy.cache_job_search_results("python developer", "San Francisco", job_results)
    
    # Test AI advice caching
    print("\n2Ô∏è‚É£ AI CAREER ADVICE CACHING (LangCache)")
    user_profile = {
        "skills": ["Python", "JavaScript", "React", "AWS"],
        "experience_years": 5,
        "industry": "Technology"
    }
    career_advice = "Focus on cloud architecture certifications and contribute to open source projects to advance to senior roles."
    await strategy.cache_ai_career_advice(user_profile, "How can I get promoted?", career_advice)
    
    # Test session caching
    print("\n3Ô∏è‚É£ USER SESSION CACHING (Memory/Redis)")
    session_data = {
        "last_search": "python developer",
        "preferences": {"remote": True, "salary_min": 100000},
        "search_history": ["python", "javascript", "react"]
    }
    await strategy.cache_user_session_data("user_123", session_data)
    
    # Test company insights
    print("\n4Ô∏è‚É£ COMPANY INSIGHTS CACHING (Redis Cloud)")
    company_insights = {
        "culture_score": 4.5,
        "salary_range": {"min": 90000, "max": 150000},
        "tech_stack": ["Python", "Kubernetes", "PostgreSQL"],
        "growth_stage": "Series B"
    }
    await strategy.cache_company_insights("TechCorp", company_insights)
    
    # Test resume analysis
    print("\n5Ô∏è‚É£ RESUME ANALYSIS CACHING (Redis Cloud)")
    resume_analysis = {
        "skills": ["Python", "Machine Learning", "AWS", "Docker"],
        "experience_years": 7,
        "industry_match": "Technology - 95%",
        "recommendations": ["Add cloud certifications", "Highlight leadership experience"]
    }
    await strategy.cache_resume_analysis("abc123def456", resume_analysis)
    
    # Performance summary
    print("\n" + "=" * 60)
    print(" CACHING STRATEGY SUMMARY:")
    print(" Job searches: 2 hour TTL (data freshness)")
    print(" AI advice: 7 day TTL (advice stays relevant)")
    print(" User sessions: 1 hour TTL (active session)")
    print(" Company data: 24 hour TTL (moderate changes)")
    print(" Resume analysis: 30 day TTL (until resume changes)")
    
    # Cost savings calculation
    print("\n ESTIMATED COST SAVINGS:")
    print(" Job search API calls: 80% reduction")
    print(" OpenAI API calls: 60-75% reduction") 
    print(" Database queries: 90% reduction")
    print(" Response times: 10x faster")
    
    stats = strategy.cache.get_cache_stats()
    print(f"\n CACHE STATUS:")
    print(f"   Redis Cloud: {' Connected' if stats['redis_enabled'] else ' Offline'}")
    print(f"   LangCache: {' Enabled' if stats['langcache_enabled'] else ' Disabled'}")
    print(f"   Fallback:  Always available")

if __name__ == "__main__":
    asyncio.run(demonstrate_optimized_caching())